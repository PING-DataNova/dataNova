"""
Dashboard de m√©triques pour le LLM Judge

Permet de visualiser et analyser les performances du Judge
"""

from typing import Dict, List, Any, Optional
from datetime import datetime, timezone
from pathlib import Path
import json
from collections import defaultdict

from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich import box
from rich.text import Text


console = Console()


class JudgeMetricsDashboard:
    """
    Dashboard des performances du Judge avec visualisation Rich
    """
    
    def __init__(self, results_dir: Optional[Path] = None):
        """
        Initialise le dashboard
        
        Args:
            results_dir: R√©pertoire contenant les r√©sultats d'√©valuation (JSON)
        """
        self.results_dir = results_dir or Path(__file__).parent
        self.evaluations: List[Dict[str, Any]] = []
        self.load_evaluations()
    
    def load_evaluations(self):
        """Charge tous les r√©sultats d'√©valuation disponibles"""
        # Charger judge_result.json s'il existe
        result_file = self.results_dir / "judge_result.json"
        if result_file.exists():
            with open(result_file, 'r', encoding='utf-8') as f:
                result = json.load(f)
                self.evaluations.append(result)
        
        # Charger d'autres fichiers de r√©sultats (pattern: judge_result_*.json)
        for result_file in self.results_dir.glob("judge_result_*.json"):
            with open(result_file, 'r', encoding='utf-8') as f:
                result = json.load(f)
                self.evaluations.append(result)
    
    def show_metrics(self):
        """Affiche les m√©triques cl√©s (interface simple)"""
        if not self.evaluations:
            print("‚ö†Ô∏è  Aucune √©valuation trouv√©e")
            return
        
        total = len(self.evaluations)
        avg_score = sum(e['judge_evaluation']['overall_quality_score'] for e in self.evaluations) / total
        avg_confidence = sum(e['judge_evaluation']['overall_confidence'] for e in self.evaluations) / total
        
        print("üìä M√âTRIQUES JUDGE")
        print(f"   √âvaluations: {total}")
        print(f"   Score moyen: {avg_score:.2f}/10")
        print(f"   Confiance moyenne: {avg_confidence:.1%}")
        
        # D√©cisions
        decisions = defaultdict(int)
        for e in self.evaluations:
            decisions[e['judge_evaluation']['action_recommended']] += 1
        
        for decision, count in decisions.items():
            print(f"   {decision}: {count} ({count/total*100:.1f}%)")
    
    def display_full_dashboard(self):
        """Affiche le dashboard complet avec Rich"""
        console.clear()
        console.print("\n")
        console.print("=" * 100, style="bold cyan")
        console.print("üìä DASHBOARD M√âTRIQUES LLM JUDGE", style="bold cyan", justify="center")
        console.print("=" * 100, style="bold cyan")
        console.print("\n")
        
        if not self.evaluations:
            console.print("‚ö†Ô∏è  Aucune √©valuation trouv√©e", style="yellow")
            console.print(f"   Recherche dans: {self.results_dir}")
            return
        
        # 1. Vue d'ensemble
        self._display_overview()
        
        # 2. Scores par crit√®re
        self._display_criteria_scores()
        
        # 3. D√©cisions
        self._display_decisions_summary()
        
        # 4. Performance par type d'√©v√©nement
        self._display_event_type_performance()
        
        # 5. Timeline des √©valuations
        self._display_timeline()
        
        console.print("\n" + "=" * 100, style="bold cyan")
    
    def _display_overview(self):
        """Affiche la vue d'ensemble"""
        total = len(self.evaluations)
        
        # Calculer les statistiques globales
        avg_overall_score = sum(
            e['judge_evaluation']['overall_quality_score'] 
            for e in self.evaluations
        ) / total if total > 0 else 0
        
        avg_confidence = sum(
            e['judge_evaluation']['overall_confidence'] 
            for e in self.evaluations
        ) / total if total > 0 else 0
        
        # Cr√©er le tableau de vue d'ensemble
        table = Table(title="üìà Vue d'Ensemble", box=box.ROUNDED, show_header=True)
        table.add_column("M√©trique", style="cyan", width=30)
        table.add_column("Valeur", style="bold green", justify="right", width=20)
        
        table.add_row("Nombre d'√©valuations", str(total))
        table.add_row("Score moyen global", f"{avg_overall_score:.2f}/10")
        table.add_row("Confiance moyenne", f"{avg_confidence:.2%}")
        
        # Score Pertinence Checker moyen
        avg_pertinence = sum(
            e['judge_evaluation']['pertinence_checker_evaluation']['weighted_score']
            for e in self.evaluations
        ) / total if total > 0 else 0
        
        # Score Risk Analyzer moyen
        avg_risk = sum(
            e['judge_evaluation']['risk_analyzer_evaluation']['weighted_score']
            for e in self.evaluations
        ) / total if total > 0 else 0
        
        table.add_row("Score Pertinence Checker", f"{avg_pertinence:.2f}/10")
        table.add_row("Score Risk Analyzer", f"{avg_risk:.2f}/10")
        
        console.print(table)
        console.print("\n")
    
    def _display_criteria_scores(self):
        """Affiche les scores d√©taill√©s par crit√®re"""
        if not self.evaluations:
            return
        
        # Extraire tous les crit√®res
        criteria_scores = defaultdict(list)
        
        for eval_result in self.evaluations:
            # Pertinence Checker
            pc_eval = eval_result['judge_evaluation']['pertinence_checker_evaluation']
            for criterion, data in pc_eval.items():
                if isinstance(data, dict) and 'score' in data:
                    criteria_scores[f"PC: {criterion}"].append(data['score'])
            
            # Risk Analyzer
            ra_eval = eval_result['judge_evaluation']['risk_analyzer_evaluation']
            for criterion, data in ra_eval.items():
                if isinstance(data, dict) and 'score' in data:
                    criteria_scores[f"RA: {criterion}"].append(data['score'])
        
        # Cr√©er le tableau
        table = Table(title="üìä Scores par Crit√®re", box=box.ROUNDED, show_header=True)
        table.add_column("Crit√®re", style="cyan", width=40)
        table.add_column("Moyenne", style="yellow", justify="right", width=10)
        table.add_column("Min", style="red", justify="right", width=8)
        table.add_column("Max", style="green", justify="right", width=8)
        table.add_column("Barre", width=30)
        
        # Trier par moyenne d√©croissante
        sorted_criteria = sorted(
            criteria_scores.items(),
            key=lambda x: sum(x[1]) / len(x[1]),
            reverse=True
        )
        
        for criterion, scores in sorted_criteria:
            avg = sum(scores) / len(scores)
            min_score = min(scores)
            max_score = max(scores)
            
            # Barre de progression
            bar_length = int((avg / 10) * 20)
            bar = "‚ñà" * bar_length + "‚ñë" * (20 - bar_length)
            
            # Couleur selon le score
            if avg >= 8:
                bar_color = "green"
            elif avg >= 6:
                bar_color = "yellow"
            else:
                bar_color = "red"
            
            table.add_row(
                criterion.replace("_", " ").title(),
                f"{avg:.1f}",
                f"{min_score}",
                f"{max_score}",
                Text(bar, style=bar_color)
            )
        
        console.print(table)
        console.print("\n")
    
    def _display_decisions_summary(self):
        """Affiche le r√©sum√© des d√©cisions"""
        decisions = defaultdict(int)
        
        for eval_result in self.evaluations:
            action = eval_result['judge_evaluation']['action_recommended']
            decisions[action] += 1
        
        total = len(self.evaluations)
        
        # Cr√©er le tableau
        table = Table(title="üö¶ R√©partition des D√©cisions", box=box.ROUNDED, show_header=True)
        table.add_column("D√©cision", style="cyan", width=20)
        table.add_column("Nombre", style="yellow", justify="right", width=10)
        table.add_column("Pourcentage", style="green", justify="right", width=15)
        table.add_column("Barre", width=40)
        
        # Ordre de priorit√©
        decision_order = ["APPROVE", "REVIEW", "REVIEW_PRIORITY", "REJECT"]
        decision_colors = {
            "APPROVE": "green",
            "REVIEW": "yellow",
            "REVIEW_PRIORITY": "orange3",
            "REJECT": "red"
        }
        
        for decision in decision_order:
            count = decisions.get(decision, 0)
            percentage = (count / total * 100) if total > 0 else 0
            
            # Barre de progression
            bar_length = int((count / total) * 30) if total > 0 else 0
            bar = "‚ñà" * bar_length + "‚ñë" * (30 - bar_length)
            
            table.add_row(
                decision,
                str(count),
                f"{percentage:.1f}%",
                Text(bar, style=decision_colors.get(decision, "white"))
            )
        
        console.print(table)
        console.print("\n")
    
    def _display_event_type_performance(self):
        """Affiche les performances par type d'√©v√©nement"""
        event_stats = defaultdict(lambda: {"scores": [], "decisions": defaultdict(int)})
        
        for eval_result in self.evaluations:
            event_type = eval_result.get('event_type', 'unknown')
            score = eval_result['judge_evaluation']['overall_quality_score']
            decision = eval_result['judge_evaluation']['action_recommended']
            
            event_stats[event_type]["scores"].append(score)
            event_stats[event_type]["decisions"][decision] += 1
        
        # Cr√©er le tableau
        table = Table(title="üåç Performance par Type d'√âv√©nement", box=box.ROUNDED, show_header=True)
        table.add_column("Type", style="cyan", width=20)
        table.add_column("√âvaluations", style="yellow", justify="right", width=12)
        table.add_column("Score Moyen", style="green", justify="right", width=15)
        table.add_column("D√©cision Dominante", style="magenta", width=20)
        
        for event_type, stats in event_stats.items():
            count = len(stats["scores"])
            avg_score = sum(stats["scores"]) / count if count > 0 else 0
            
            # D√©cision la plus fr√©quente
            dominant_decision = max(stats["decisions"].items(), key=lambda x: x[1])[0] if stats["decisions"] else "N/A"
            
            table.add_row(
                event_type.capitalize(),
                str(count),
                f"{avg_score:.2f}/10",
                dominant_decision
            )
        
        console.print(table)
        console.print("\n")
    
    def _display_timeline(self):
        """Affiche la timeline des √©valuations"""
        if not self.evaluations:
            return
        
        # Cr√©er le tableau
        table = Table(title="‚è∞ Timeline des √âvaluations", box=box.ROUNDED, show_header=True)
        table.add_column("Date", style="cyan", width=25)
        table.add_column("Event ID", style="yellow", width=25)
        table.add_column("Type", style="magenta", width=15)
        table.add_column("Score", style="green", justify="right", width=10)
        table.add_column("D√©cision", style="bold", width=15)
        
        # Trier par timestamp
        sorted_evals = sorted(
            self.evaluations,
            key=lambda x: x['judge_evaluation']['metadata'].get('evaluation_timestamp', ''),
            reverse=True
        )
        
        for eval_result in sorted_evals[:10]:  # Afficher les 10 derniers
            timestamp = eval_result['judge_evaluation']['metadata'].get('evaluation_timestamp', 'N/A')
            event_id = eval_result.get('event_id', 'unknown')[:20]
            event_type = eval_result.get('event_type', 'unknown')
            score = eval_result['judge_evaluation']['overall_quality_score']
            decision = eval_result['judge_evaluation']['action_recommended']
            
            # Couleur de la d√©cision
            decision_colors = {
                "APPROVE": "green",
                "REVIEW": "yellow",
                "REVIEW_PRIORITY": "orange3",
                "REJECT": "red"
            }
            
            table.add_row(
                timestamp[:19] if timestamp != 'N/A' else 'N/A',
                event_id,
                event_type,
                f"{score:.2f}",
                Text(decision, style=decision_colors.get(decision, "white"))
            )
        
        console.print(table)
        console.print("\n")
    
    def export_metrics(self, output_file: Path):
        """
        Exporte les m√©triques au format JSON
        
        Args:
            output_file: Fichier de sortie
        """
        metrics = {
            "export_timestamp": datetime.now(timezone.utc).isoformat(),
            "total_evaluations": len(self.evaluations),
            "overview": self._compute_overview_metrics(),
            "criteria_scores": self._compute_criteria_metrics(),
            "decisions": self._compute_decision_metrics(),
            "event_types": self._compute_event_type_metrics()
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, indent=2, ensure_ascii=False)
        
        console.print(f"‚úÖ M√©triques export√©es vers: {output_file}", style="green")
    
    def _compute_overview_metrics(self) -> Dict:
        """Calcule les m√©triques de vue d'ensemble"""
        if not self.evaluations:
            return {}
        
        total = len(self.evaluations)
        
        return {
            "avg_overall_score": sum(e['judge_evaluation']['overall_quality_score'] for e in self.evaluations) / total,
            "avg_confidence": sum(e['judge_evaluation']['overall_confidence'] for e in self.evaluations) / total,
            "avg_pertinence_score": sum(e['judge_evaluation']['pertinence_checker_evaluation']['weighted_score'] for e in self.evaluations) / total,
            "avg_risk_score": sum(e['judge_evaluation']['risk_analyzer_evaluation']['weighted_score'] for e in self.evaluations) / total
        }
    
    def _compute_criteria_metrics(self) -> Dict:
        """Calcule les m√©triques par crit√®re"""
        criteria_scores = defaultdict(list)
        
        for eval_result in self.evaluations:
            pc_eval = eval_result['judge_evaluation']['pertinence_checker_evaluation']
            for criterion, data in pc_eval.items():
                if isinstance(data, dict) and 'score' in data:
                    criteria_scores[f"pertinence_{criterion}"].append(data['score'])
            
            ra_eval = eval_result['judge_evaluation']['risk_analyzer_evaluation']
            for criterion, data in ra_eval.items():
                if isinstance(data, dict) and 'score' in data:
                    criteria_scores[f"risk_{criterion}"].append(data['score'])
        
        return {
            criterion: {
                "average": sum(scores) / len(scores),
                "min": min(scores),
                "max": max(scores),
                "count": len(scores)
            }
            for criterion, scores in criteria_scores.items()
        }
    
    def _compute_decision_metrics(self) -> Dict:
        """Calcule les m√©triques de d√©cision"""
        decisions = defaultdict(int)
        
        for eval_result in self.evaluations:
            action = eval_result['judge_evaluation']['action_recommended']
            decisions[action] += 1
        
        total = len(self.evaluations)
        
        return {
            decision: {
                "count": count,
                "percentage": (count / total * 100) if total > 0 else 0
            }
            for decision, count in decisions.items()
        }
    
    def _compute_event_type_metrics(self) -> Dict:
        """Calcule les m√©triques par type d'√©v√©nement"""
        event_stats = defaultdict(lambda: {"scores": [], "decisions": defaultdict(int)})
        
        for eval_result in self.evaluations:
            event_type = eval_result.get('event_type', 'unknown')
            score = eval_result['judge_evaluation']['overall_quality_score']
            decision = eval_result['judge_evaluation']['action_recommended']
            
            event_stats[event_type]["scores"].append(score)
            event_stats[event_type]["decisions"][decision] += 1
        
        return {
            event_type: {
                "count": len(stats["scores"]),
                "avg_score": sum(stats["scores"]) / len(stats["scores"]) if stats["scores"] else 0,
                "decisions": dict(stats["decisions"])
            }
            for event_type, stats in event_stats.items()
        }


def main():
    """Point d'entr√©e principal pour afficher le dashboard"""
    dashboard = JudgeMetricsDashboard()
    dashboard.display_full_dashboard()
    
    # Exporter les m√©triques
    output_file = Path(__file__).parent / "judge_metrics_export.json"
    dashboard.export_metrics(output_file)


if __name__ == "__main__":
    main()