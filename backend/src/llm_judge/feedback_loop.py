"""
Syst√®me de feedback loop pour l'am√©lioration continue du Judge
"""

from typing import Dict, List, Any
from datetime import datetime
from collections import defaultdict


class JudgeFeedbackLoop:
    """
    Syst√®me d'am√©lioration continue du Judge bas√© sur les validations humaines
    """
    
    def __init__(self):
        """
        Initialise le feedback loop
        """
        self.disagreements: List[Dict[str, Any]] = []
        self.ground_truth_cases: List[Dict[str, Any]] = []
        self.adjustments_history: List[Dict[str, Any]] = []
    
    def log_disagreement(
        self,
        case_id: str,
        judge_decision: str,
        human_decision: str,
        human_reasoning: str,
        judge_score: float,
        human_score: float = None
    ):
        """
        Enregistre un d√©saccord entre le Judge et l'humain
        
        Args:
            case_id: Identifiant du cas
            judge_decision: D√©cision du Judge (APPROVE/REVIEW/REJECT)
            human_decision: D√©cision de l'humain
            human_reasoning: Raisonnement de l'humain
            judge_score: Score global du Judge
            human_score: Score global de l'humain (optionnel)
        """
        disagreement = {
            "case_id": case_id,
            "judge_decision": judge_decision,
            "human_decision": human_decision,
            "human_reasoning": human_reasoning,
            "judge_score": judge_score,
            "human_score": human_score,
            "timestamp": datetime.utcnow().isoformat() + "Z"
        }
        
        self.disagreements.append(disagreement)
        
        print(f"‚ö†Ô∏è  D√©saccord enregistr√© pour {case_id}:")
        print(f"   Judge: {judge_decision} (score: {judge_score})")
        print(f"   Humain: {human_decision}")
        
        # Tous les 10 d√©saccords, analyser et ajuster
        if len(self.disagreements) % 10 == 0:
            print(f"\nüîÑ Analyse de {len(self.disagreements)} d√©saccords...")
            self.analyze_and_adjust()
    
    def log_agreement(
        self,
        case_id: str,
        decision: str,
        score: float
    ):
        """
        Enregistre un accord entre le Judge et l'humain
        
        Args:
            case_id: Identifiant du cas
            decision: D√©cision commune
            score: Score du Judge
        """
        self.ground_truth_cases.append({
            "case_id": case_id,
            "decision": decision,
            "score": score,
            "timestamp": datetime.utcnow().isoformat() + "Z"
        })
    
    def analyze_and_adjust(self):
        """
        Analyse les d√©saccords et propose des ajustements
        """
        if len(self.disagreements) < 10:
            print("   ‚è≥ Pas assez de d√©saccords pour analyser (minimum 10)")
            return
        
        # Analyser les 10 derniers d√©saccords
        recent_disagreements = self.disagreements[-10:]
        
        # Identifier les patterns
        patterns = self._identify_patterns(recent_disagreements)
        
        print(f"\nüìä Patterns identifi√©s:")
        for pattern in patterns:
            print(f"   - {pattern['type']}: {pattern['count']} cas")
            print(f"     Crit√®re: {pattern.get('criterion', 'N/A')}")
            print(f"     Recommandation: {pattern['recommendation']}")
        
        # Enregistrer les ajustements
        adjustment = {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "disagreements_analyzed": len(recent_disagreements),
            "patterns": patterns
        }
        
        self.adjustments_history.append(adjustment)
    
    def _identify_patterns(self, disagreements: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Identifie les patterns dans les d√©saccords
        
        Args:
            disagreements: Liste des d√©saccords r√©cents
            
        Returns:
            Liste des patterns identifi√©s
        """
        patterns = []
        
        # Pattern 1 : Judge trop optimiste (APPROVE ‚Üí REJECT/REVIEW)
        over_optimistic = [
            d for d in disagreements
            if d['judge_decision'] == 'APPROVE' and d['human_decision'] in ['REJECT', 'REVIEW']
        ]
        
        if len(over_optimistic) >= 3:
            patterns.append({
                "type": "over_optimistic",
                "count": len(over_optimistic),
                "recommendation": "Augmenter les seuils de d√©cision (8.5 ‚Üí 9.0 pour APPROVE)"
            })
        
        # Pattern 2 : Judge trop strict (REJECT ‚Üí APPROVE/REVIEW)
        under_optimistic = [
            d for d in disagreements
            if d['judge_decision'] == 'REJECT' and d['human_decision'] in ['APPROVE', 'REVIEW']
        ]
        
        if len(under_optimistic) >= 3:
            patterns.append({
                "type": "under_optimistic",
                "count": len(under_optimistic),
                "recommendation": "R√©duire les seuils de d√©cision (7.0 ‚Üí 6.5 pour REJECT)"
            })
        
        # Pattern 3 : Scores trop √©lev√©s
        score_diff = [
            d for d in disagreements
            if d.get('human_score') and d['judge_score'] - d['human_score'] > 1.5
        ]
        
        if len(score_diff) >= 3:
            patterns.append({
                "type": "score_inflation",
                "count": len(score_diff),
                "recommendation": "Ajuster les prompts pour √™tre plus critique"
            })
        
        # Pattern 4 : Scores trop bas
        score_deflation = [
            d for d in disagreements
            if d.get('human_score') and d['human_score'] - d['judge_score'] > 1.5
        ]
        
        if len(score_deflation) >= 3:
            patterns.append({
                "type": "score_deflation",
                "count": len(score_deflation),
                "recommendation": "Ajuster les prompts pour √™tre moins strict"
            })
        
        return patterns
    
    def calculate_judge_accuracy(self) -> float:
        """
        Calcule la pr√©cision du Judge par rapport aux validations humaines
        
        Returns:
            Pr√©cision (0-1)
        """
        total_cases = len(self.disagreements) + len(self.ground_truth_cases)
        
        if total_cases == 0:
            return 1.0
        
        correct_decisions = len(self.ground_truth_cases)
        
        return round(correct_decisions / total_cases, 3)
    
    def get_metrics(self) -> Dict[str, Any]:
        """
        R√©cup√®re les m√©triques de performance du Judge
        
        Returns:
            Dictionnaire des m√©triques
        """
        total_cases = len(self.disagreements) + len(self.ground_truth_cases)
        
        if total_cases == 0:
            return {
                "total_cases": 0,
                "judge_accuracy": 1.0,
                "false_approve_rate": 0.0,
                "false_reject_rate": 0.0,
                "total_adjustments": 0
            }
        
        # False Approve Rate : APPROVE ‚Üí REJECT/REVIEW
        false_approves = len([
            d for d in self.disagreements
            if d['judge_decision'] == 'APPROVE' and d['human_decision'] in ['REJECT', 'REVIEW']
        ])
        
        # False Reject Rate : REJECT ‚Üí APPROVE/REVIEW
        false_rejects = len([
            d for d in self.disagreements
            if d['judge_decision'] == 'REJECT' and d['human_decision'] in ['APPROVE', 'REVIEW']
        ])
        
        return {
            "total_cases": total_cases,
            "agreements": len(self.ground_truth_cases),
            "disagreements": len(self.disagreements),
            "judge_accuracy": self.calculate_judge_accuracy(),
            "false_approve_rate": round(false_approves / total_cases, 3),
            "false_reject_rate": round(false_rejects / total_cases, 3),
            "total_adjustments": len(self.adjustments_history)
        }
