"""
Agent 3 : LLM Judge - Ã‰valuation de la qualitÃ© des analyses (Anthropic Claude)
"""

import json
import time
from typing import Dict, List, Any
from datetime import datetime
from anthropic import Anthropic

from .criteria_evaluator import CriteriaEvaluator
from .weights_config import get_weights, calculate_weighted_score
from .prompts import JUDGE_SYSTEM_PROMPT, FINAL_DECISION_PROMPT


class Judge:
    """
    Agent Judge qui Ã©value la qualitÃ© des analyses de pertinence et de risque
    """
    
    def __init__(self, llm_model: str = "claude-sonnet-4-20250514"):
        """
        Initialise le Judge
        
        Args:
            llm_model: ModÃ¨le Claude Ã  utiliser
        """
        self.llm_model = llm_model
        self.client = Anthropic()
        self.evaluator = CriteriaEvaluator(llm_model)
    
    def evaluate(
        self,
        document: Dict[str, Any],
        pertinence_result: Dict[str, Any],
        risk_analysis: Dict[str, Any],
        sites: List[Dict],
        suppliers: List[Dict],
        supplier_relationships: List[Dict]
    ) -> Dict[str, Any]:
        """
        Ã‰value la qualitÃ© complÃ¨te de l'analyse (Pertinence + Risque)
        
        Args:
            document: Document source
            pertinence_result: RÃ©sultat de l'analyse de pertinence (Agent 1B)
            risk_analysis: RÃ©sultat de l'analyse de risque (Agent 2)
            sites: Liste des sites Hutchinson
            suppliers: Liste des fournisseurs
            supplier_relationships: Liste des relations site-fournisseur
            
        Returns:
            Ã‰valuation complÃ¨te avec scores, dÃ©cision et mÃ©tadonnÃ©es
        """
        print(f"\nðŸŽ¯ Ã‰valuation Judge pour document: {document.get('id', 'unknown')}")
        print(f"   Type d'Ã©vÃ©nement: {document.get('event_type', 'unknown')}")
        
        # 1. Ã‰valuer le Pertinence Checker (Agent 1B)
        print("\nðŸ“‹ Ã‰valuation du Pertinence Checker...")
        pertinence_eval = self.evaluator.evaluate_pertinence_checker(
            document=document,
            pertinence_result=pertinence_result,
            sites_count=len(sites),
            suppliers_count=len(suppliers)
        )
        
        # Calculer le score pondÃ©rÃ© pour Pertinence Checker
        event_type = document.get('event_type', 'climatique')
        weights = get_weights(event_type)
        
        pertinence_scores = self.evaluator.extract_scores(pertinence_eval)
        # Filtrer uniquement les critÃ¨res applicables au Pertinence Checker
        pertinence_weights = {k: v for k, v in weights.items() if k in pertinence_scores}
        pertinence_weighted_score = calculate_weighted_score(pertinence_scores, pertinence_weights)
        pertinence_confidence = self.evaluator.calculate_average_confidence(pertinence_eval)
        
        pertinence_eval['weighted_score'] = pertinence_weighted_score
        pertinence_eval['confidence_overall'] = pertinence_confidence
        
        print(f"   âœ… Score pondÃ©rÃ©: {pertinence_weighted_score}/10")
        print(f"   âœ… Confiance: {pertinence_confidence}")
        
        # DÃ©lai entre les appels API pour Ã©viter le rate limit
        print("   â³ Pause de 5s avant l'Ã©valuation du Risk Analyzer...")
        time.sleep(5)
        
        # 2. Ã‰valuer le Risk Analyzer (Agent 2)
        print("\nðŸ“Š Ã‰valuation du Risk Analyzer...")
        risk_eval = self.evaluator.evaluate_risk_analyzer(
            document=document,
            pertinence_result=pertinence_result,
            risk_analysis=risk_analysis,
            sites_count=len(sites),
            suppliers_count=len(suppliers),
            relationships_count=len(supplier_relationships)
        )
        
        # Calculer le score pondÃ©rÃ© pour Risk Analyzer
        risk_scores = self.evaluator.extract_scores(risk_eval)
        risk_weighted_score = calculate_weighted_score(risk_scores, weights)
        risk_confidence = self.evaluator.calculate_average_confidence(risk_eval)
        
        risk_eval['weighted_score'] = risk_weighted_score
        risk_eval['confidence_overall'] = risk_confidence
        
        print(f"   âœ… Score pondÃ©rÃ©: {risk_weighted_score}/10")
        print(f"   âœ… Confiance: {risk_confidence}")
        
        # 3. Calculer le score global
        overall_score = round((pertinence_weighted_score + risk_weighted_score) / 2, 2)
        overall_confidence = round((pertinence_confidence + risk_confidence) / 2, 2)
        
        print(f"\nðŸŽ¯ Score global: {overall_score}/10")
        print(f"ðŸŽ¯ Confiance globale: {overall_confidence}")
        
        # 4. DÃ©terminer l'action recommandÃ©e
        print("\nðŸ¤” DÃ©termination de l'action...")
        decision = self._determine_action(
            pertinence_weighted_score,
            pertinence_confidence,
            risk_weighted_score,
            risk_confidence,
            overall_score,
            overall_confidence
        )
        
        print(f"   âœ… Action: {decision['action_recommended']}")
        print(f"   ðŸ“ Raisonnement: {decision['reasoning']}")
        
        # 5. Construire le rÃ©sultat final
        result = {
            "event_id": document.get('id', 'unknown'),
            "event_type": event_type,
            "judge_evaluation": {
                "pertinence_checker_evaluation": pertinence_eval,
                "risk_analyzer_evaluation": risk_eval,
                "overall_quality_score": overall_score,
                "overall_confidence": overall_confidence,
                "action_recommended": decision['action_recommended'],
                "reasoning": decision['reasoning'],
                "metadata": {
                    "judge_model": self.llm_model,
                    "evaluation_timestamp": datetime.utcnow().isoformat() + "Z",
                    "weights_used": event_type,
                    "total_criteria_evaluated": len(pertinence_scores) + len(risk_scores)
                }
            }
        }
        
        return result
    
    def _determine_action(
        self,
        pertinence_score: float,
        pertinence_confidence: float,
        risk_score: float,
        risk_confidence: float,
        overall_score: float,
        overall_confidence: float
    ) -> Dict[str, str]:
        """
        DÃ©termine l'action recommandÃ©e basÃ©e sur les scores et la confiance
        
        Args:
            pertinence_score: Score pondÃ©rÃ© du Pertinence Checker
            pertinence_confidence: Confiance du Pertinence Checker
            risk_score: Score pondÃ©rÃ© du Risk Analyzer
            risk_confidence: Confiance du Risk Analyzer
            overall_score: Score global
            overall_confidence: Confiance globale
            
        Returns:
            Dictionnaire avec action_recommended et reasoning
        """
        prompt = FINAL_DECISION_PROMPT.format(
            pertinence_score=pertinence_score,
            pertinence_confidence=pertinence_confidence,
            risk_score=risk_score,
            risk_confidence=risk_confidence,
            overall_score=overall_score,
            overall_confidence=overall_confidence
        )
        
        response = self.client.messages.create(
            model=self.llm_model,
            max_tokens=1024,
            temperature=0.0,  # DÃ©terministe pour la dÃ©cision
            system=JUDGE_SYSTEM_PROMPT,
            messages=[
                {"role": "user", "content": prompt}
            ]
        )
        
        result_text = response.content[0].text.strip()
        
        # Parser le JSON
        try:
            if "```json" in result_text:
                result_text = result_text.split("```json")[1].split("```")[0].strip()
            elif "```" in result_text:
                result_text = result_text.split("```")[1].split("```")[0].strip()
            
            decision = json.loads(result_text)
            return decision
        except json.JSONDecodeError:
            # DÃ©cision par dÃ©faut basÃ©e sur les rÃ¨gles
            if overall_score >= 8.5 and overall_confidence >= 0.85:
                action = "APPROVE"
                reasoning = f"Score global de {overall_score} (â‰¥ 8.5) avec confiance Ã©levÃ©e ({overall_confidence} â‰¥ 0.85)."
            elif overall_score >= 8.5 and overall_confidence < 0.85:
                action = "REVIEW"
                reasoning = f"Score global Ã©levÃ© ({overall_score}) mais confiance faible ({overall_confidence} < 0.85)."
            elif 7.0 <= overall_score < 8.5 and overall_confidence >= 0.80:
                action = "REVIEW"
                reasoning = f"Score global acceptable ({overall_score}) avec confiance correcte ({overall_confidence})."
            elif 7.0 <= overall_score < 8.5 and overall_confidence < 0.80:
                action = "REVIEW_PRIORITY"
                reasoning = f"Score global acceptable ({overall_score}) mais confiance faible ({overall_confidence})."
            else:
                action = "REJECT"
                reasoning = f"Score global insuffisant ({overall_score} < 7.0)."
            
            return {
                "action_recommended": action,
                "reasoning": reasoning
            }
